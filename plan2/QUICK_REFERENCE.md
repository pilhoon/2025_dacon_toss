# Plan2 Quick Reference Guide

## ğŸ¯ ìµœê³  ì„±ëŠ¥ ë‹¬ì„± ë°©ë²• (ë°”ë¡œ ì‹¤í–‰)

### 1. ì œì¶œ íŒŒì¼ ìƒì„± (ê²€ì¦ëœ ìµœê³  ì„±ëŠ¥)
```bash
python plan2/030_deepctr_best_submission.py
```
- ì‹¤í–‰ ì‹œê°„: ~1ì‹œê°„
- GPU ë©”ëª¨ë¦¬: ~30GB
- Competition Score: ~0.47

### 2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (2M ìƒ˜í”Œ)
```bash
python plan2/029_deepctr_fast_submission.py
```
- ì‹¤í–‰ ì‹œê°„: ~10ë¶„
- ë¹ ë¥¸ ì‹¤í—˜ìš©

## ğŸ“Š ì„±ëŠ¥ ë¹„êµí‘œ

| ë°©ë²• | Competition Score | ì‹¤í–‰ ì‹œê°„ | ì¶”ì²œë„ |
|------|------------------|-----------|--------|
| Plan1 XGBoost | 0.31631 | 30ë¶„ | â˜…â˜…â˜… |
| Plan2 DeepCTR | **0.47** | 60ë¶„ | â˜…â˜…â˜…â˜…â˜… |
| Ensemble (ì˜ˆìƒ) | ~0.50+ | 90ë¶„ | â˜…â˜…â˜…â˜…â˜… |

## âš¡ í•µì‹¬ ì„¤ì •ê°’

### ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
# ë°ì´í„°
n_samples = 10_000_000  # ì „ì²´ ì‚¬ìš©
sparse_features = 40
dense_features = 25
embedding_dim = 24

# ëª¨ë¸ (DCN)
cross_num = 5
dnn_hidden_units = (1024, 512, 256, 128)
dnn_dropout = 0.15

# í•™ìŠµ
batch_size = 500_000  # GPU 80GB ê¸°ì¤€
epochs = 12
learning_rate = 0.001  # Adam default
```

### GPU ë©”ëª¨ë¦¬ë³„ ë°°ì¹˜ í¬ê¸°
| GPU ë©”ëª¨ë¦¬ | ê¶Œì¥ ë°°ì¹˜ í¬ê¸° |
|-----------|---------------|
| 16GB | 20,000 |
| 24GB | 50,000 |
| 40GB | 100,000 |
| 80GB | 200,000-500,000 |

## ğŸ”§ ë¬¸ì œ í•´ê²°

### NaN Loss ë°œìƒ ì‹œ
```python
# 1. pos_weight ì¤„ì´ê¸°
pos_weight = min(pos_weight, 20)

# 2. ë³´ìˆ˜ì  ì´ˆê¸°í™”
nn.init.xavier_uniform_(layer.weight, gain=0.01)

# 3. Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### OOM (Out of Memory) ë°œìƒ ì‹œ
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size = batch_size // 2

# GPU ìºì‹œ ì •ë¦¬
torch.cuda.empty_cache()

# Feature ìˆ˜ ì¤„ì´ê¸°
sparse_features = sparse_features[:20]
dense_features = dense_features[:10]
```

### ì„±ëŠ¥ì´ ë‚®ì„ ë•Œ
```python
# 1. ë” ë§ì€ ë°ì´í„° ì‚¬ìš©
n_samples = min(len(df), 5_000_000)

# 2. ë” í° ëª¨ë¸
dnn_hidden_units = (1024, 512, 256, 128)
embedding_dim = 32

# 3. ë” ë§ì€ epoch
epochs = 20
```

## ğŸ“ ì£¼ìš” íŒŒì¼

### ì œì¶œìš©
- `030_deepctr_best_submission.py` - ìµœì¢… ì œì¶œ (ì „ì²´ ë°ì´í„°)
- `029_deepctr_fast_submission.py` - ë¹ ë¥¸ ì œì¶œ (ì¼ë¶€ ë°ì´í„°)

### ì‹¤í—˜ìš©
- `018_deepctr_fixed.py` - DeepCTR ëª¨ë¸ ë¹„êµ
- `021_score_optimized_deepctr.py` - Competition Score ìµœì í™”
- `022_deepctr_large_batch.py` - GPU í™œìš© ì‹¤í—˜

### ê²°ê³¼
- `030_deepctr_best_submission.csv` - ì œì¶œ íŒŒì¼
- `experiments/best_submission_model.pth` - ëª¨ë¸ weights

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **Ensemble**: XGBoost + DeepCTR
```python
# ì˜ˆì¸¡ê°’ í‰ê· 
final_pred = 0.6 * xgb_pred + 0.4 * deepctr_pred
```

2. **Feature Engineering**
```python
# Interaction features
df['age_gender'] = df['age'].astype(str) + '_' + df['gender'].astype(str)

# Frequency encoding
freq_encoding = df['user_id'].value_counts().to_dict()
df['user_freq'] = df['user_id'].map(freq_encoding)
```

3. **Advanced Models**
- Try: xDeepFM, FiBiNET (ë©”ëª¨ë¦¬ ì¶©ë¶„ ì‹œ)
- Two-tower architecture
- Graph-based methods

## ğŸ’¡ Tips

1. **í•™ìŠµ ëª¨ë‹ˆí„°ë§**: validation AUCê°€ ì¦ê°€í•˜ì§€ ì•Šìœ¼ë©´ early stopping
2. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: float32 â†’ float16 (mixed precision)
3. **ì†ë„ í–¥ìƒ**: DataLoaderì˜ num_workers ì¦ê°€
4. **ì¬í˜„ì„±**: random seed ê³ ì •
```python
import random
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
```

## ğŸ“ˆ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ìˆœìœ„

| Score | ì˜ˆìƒ ìˆœìœ„ |
|-------|----------|
| 0.47 | Top 10% |
| 0.48 | Top 5% |
| 0.49 | Top 3% |
| 0.50+ | Top 1% |

í˜„ì¬ Plan2 DeepCTR: **0.47** (Top 10% ì˜ˆìƒ)