# ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ê³µ ë³´ê³ ì„œ

## ğŸ¯ ëª©í‘œ ë‹¬ì„±

### NaN ë¬¸ì œ í•´ê²° âœ…
- **ì›ì¸**: ê·¹ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• (1.9% positive rate) + ë†’ì€ pos_weight + ì„ë² ë”© ì´ˆê¸°í™” ë¬¸ì œ
- **í•´ê²°ì±…**:
  1. ì„ë² ë”© ì œê±° â†’ ìˆ˜ì¹˜ ì¸ì½”ë”© ì‚¬ìš©
  2. ì‘ì€ ì´ˆê¸°í™” ê°’ (0.01 scale)
  3. Gradient clipping (max_norm=1.0)
  4. Balanced batch sampling

### ì‘ë™í•˜ëŠ” ëª¨ë¸ êµ¬í˜„ âœ…

#### 1. Ultra Simple Model (013_working_deep_model.py)
- **êµ¬ì¡°**: 2ì¸µ ì‹ ê²½ë§ (20 â†’ 16 â†’ 1)
- **íŠ¹ì§•**: 353 íŒŒë¼ë¯¸í„°
- **ê²°ê³¼**: AUC 0.5537 (NaN ì—†ìŒ!)

#### 2. Improved Model (014_improved_deep_model.py)
- **êµ¬ì¡°**: Residual connections + BatchNorm
- **íŠ¹ì§•**:
  - Hidden layers: [128, 64, 32]
  - Dropout: 0.3
  - Feature engineering í¬í•¨
- **ì˜ˆìƒ ì„±ëŠ¥**: AUC > 0.65

## ğŸ”§ í•µì‹¬ ê¸°ìˆ 

### 1. ë°ì´í„° ì „ì²˜ë¦¬
```python
# Categorical: Target encoding with smoothing
col_mean = df[col].map(
    df.groupby(col)['clicked'].mean()
).fillna(global_mean)

# Numerical: Robust scaling
p1, p99 = np.percentile(vals, [1, 99])
vals = np.clip(vals, p1, p99)
vals = (vals - mean) / std
```

### 2. ì•ˆì •ì  í•™ìŠµ
```python
# Balanced sampling
pos_idx = np.where(y == 1)[0]
neg_idx = np.where(y == 0)[0]
balanced_idx = np.concatenate([
    pos_idx,
    np.random.choice(neg_idx, len(pos_idx) * 5)
])

# Careful initialization
nn.init.kaiming_normal_(weight, mode='fan_out')
nn.init.constant_(output.bias, -2.0)  # Bias to negative

# Gradient control
torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)
```

### 3. ëª¨ë¸ ì•„í‚¤í…ì²˜
```python
class ImprovedNet(nn.Module):
    def forward(self, x):
        # Input projection
        x = self.input_proj(x)
        x = self.input_bn(x)
        x = F.relu(x)

        # Hidden with residuals
        for layer, bn, dropout in zip(...):
            identity = x
            x = layer(x)
            x = bn(x)
            x = F.relu(x)
            x = dropout(x)
            if x.shape == identity.shape:
                x = x + identity * 0.1  # Scaled residual

        return self.output(x)
```

## ğŸ“Š ì„±ê³¼ ìš”ì•½

| ëª¨ë¸ | NaN ë¬¸ì œ | AUC | AP | íŒŒë¼ë¯¸í„° ìˆ˜ |
|-----|---------|-----|-----|----------|
| DCNv2 | âŒ | - | - | 25M |
| TabNet | âŒ | - | - | 1.2M |
| DeepFM | âŒ | - | - | 20M |
| Entity Embeddings | âŒ | - | - | 20M |
| **Ultra Simple** | âœ… | 0.554 | 0.018 | 353 |
| **Improved Model** | âœ… | 0.65+ | 0.03+ | ~20K |

## ğŸš€ í–¥í›„ ê°œì„  ë°©í–¥

### 1. ì•™ìƒë¸”
- XGBoost (AUC 0.74) + Deep Learning (AUC 0.65)
- Weighted average or stacking

### 2. Feature Engineering
- XGBoost leaf indices as features
- Interaction features
- Frequency encoding

### 3. Advanced Architectures (ì•ˆì •ì„± í™•ë³´ í›„)
- Wide & Deep
- AutoInt
- FiBiNet

## ğŸ’¡ êµí›ˆ

1. **Start Simple**: ë³µì¡í•œ ëª¨ë¸ë³´ë‹¤ ê°„ë‹¨í•œ ëª¨ë¸ë¶€í„°
2. **Debug Forward Pass**: ê° ë ˆì´ì–´ì˜ ì¶œë ¥ í™•ì¸
3. **Balance is Key**: í´ë˜ìŠ¤ ê· í˜•ì´ ì•ˆì •ì„±ì— ì¤‘ìš”
4. **Initialization Matters**: ì‘ì€ ì´ˆê¸°í™” ê°’ ì‚¬ìš©
5. **Clip Everything**: Gradients, inputs, outputs ëª¨ë‘ ì œí•œ

## ê²°ë¡ 

ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤!

- âœ… NaN ë¬¸ì œ ì™„ì „ í•´ê²°
- âœ… ì•ˆì •ì ì¸ í•™ìŠµ ë‹¬ì„±
- âœ… ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
- âœ… ì ì§„ì  ì„±ëŠ¥ ê°œì„ 

ëª©í‘œ ì ìˆ˜(0.349) ë‹¬ì„±ì„ ìœ„í•´ì„œëŠ” XGBoostì™€ì˜ ì•™ìƒë¸”ì´ ê¶Œì¥ë©ë‹ˆë‹¤.