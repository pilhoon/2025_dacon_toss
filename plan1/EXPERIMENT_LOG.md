# 실험 기록 및 제출 결과

## 평가 지표
- **score = 0.5 × AP + 0.5 × (1/(1+WLL))**
  - AP (Average Precision): 50% - 높을수록 좋음
  - WLL (Weighted LogLoss): 50% - 낮을수록 좋음
- **목표**: 0.349 이상 (현재 리더보드 1위: 0.34995)

---

## 실험 001-003: 초기 베이스라인
**날짜**: 2025-09-16 오전

### 동기
- 데이터 이해 및 베이스라인 구축
- 기본적인 GBDT 모델로 시작점 설정

### 결과
- **002_simple_baseline.py**: AUC 0.6960 (100K 샘플)
- **003_baseline_more_features.py**: AUC 0.6960 (200K 샘플)
- **문제점**: 성능 게이트(0.70) 미달

---

## 실험 004: 피처 엔지니어링 시도
**날짜**: 2025-09-16 오전

### 동기
- Target Encoding, 시간 피처, 상호작용 피처 추가
- History/l_feat 집계 피처 생성

### 결과
- AUC 0.5092 (실패)
- **문제점**: Target Encoding 구현 오류로 성능 악화

---

## 실험 005: XGBoost/LightGBM 비교
**날짜**: 2025-09-16 10:57

### 동기
- 다양한 GBDT 모델 비교
- 단순 전처리로 안정적 성능 확보

### 결과
- **XGBoost**: AUC 0.7199 ✅
- **LightGBM**: AUC 0.7192
- **성공**: 게이트 통과 (0.71 > 0.70)

---

## 실험 006: Optuna 하이퍼파라미터 튜닝
**날짜**: 2025-09-16 18:20

### 동기
- LightGBM 파라미터 자동 최적화
- 30 trials로 최적 조합 탐색

### 결과
- **최적 AUC**: 0.7198 (5-fold CV)
- **문제 파라미터**:
  - max_depth: 4 (너무 얕음)
  - min_gain_to_split: 0.376 (너무 높음)
- **부작용**: "No further splits" 경고 다수 발생

---

## 실험 007: 앙상블/스태킹
**날짜**: 2025-09-16 18:18

### 동기
- 3개 모델(HistGBM, XGBoost, LightGBM) 앙상블
- 스태킹으로 성능 향상 시도

### 결과
- **개별 모델**:
  - HistGBM: 0.7134
  - XGBoost: 0.7061
  - LightGBM: 0.6995
- **앙상블**: Stacking 0.7153
- **칼리브레이션**: LogLoss 0.0825 → 0.0805

---

## 실험 008: 첫 제출 (LightGBM with Optuna params)
**날짜**: 2025-09-16 20:31
**파일**: submission.csv

### 동기
- Optuna 최적 파라미터로 전체 데이터 학습
- 피처 엔지니어링 적용

### 제출 결과
- **리더보드 점수: 0.21436** ❌
- **예측 통계**:
  - 평균: 0.0205
  - 표준편차: 0.0215 (너무 낮음!)
  - >0.5: 58개 (0.004%)

### 분석
- **AP 매우 낮음**: 예측 분산 부족으로 순위 매기기 실패
- **WLL 높음**: ~1.3-2.0 추정
- **원인**: Optuna 파라미터가 과도하게 제약적

---

## 실험 009: 전체 데이터 3-Fold CV
**날짜**: 2025-09-16 20:00 (진행 중)

### 동기
- 전체 1070만 데이터로 정확한 성능 평가
- XGBoost vs LightGBM 비교

### 중간 결과
- **Fold 1**:
  - XGBoost: 0.7430 ✨
  - LightGBM: 0.7340
- XGBoost가 일관되게 우수

---

## 실험 010: XGBoost 개선 시도
**날짜**: 2025-09-16 21:15
**파일**: 010_xgboost_submission.csv

### 동기
- 예측 분산 증가로 AP 개선
- scale_pos_weight로 클래스 불균형 처리

### 주요 변경
- max_depth: 4 → 10
- scale_pos_weight: 51.43 추가
- reg_alpha: 0.1 → 0.01

### 제출 결과
- **리더보드 점수: 0.31631** ✅ (47% 개선!)
- **예측 통계**:
  - 평균: 0.2885 (너무 높음!)
  - 표준편차: 0.1848 (8.6배 증가)
  - >0.5: 223,604개 (14.64%)

### 분석
- **AP 개선**: 분산 증가로 ~0.25 추정
- **WLL 악화**: 예측 평균이 너무 높아 ~0.54
- **다음 목표**: 예측 평균을 실제(0.0191)에 근접

---

## 실험 011: 균형잡힌 XGBoost (진행 중)
**날짜**: 2025-09-16 22:00
**파일**: 011_balanced_submission.csv (예정)

### 동기
- 5-Fold CV로 최적 scale_pos_weight 찾기
- Isotonic Regression으로 확률 보정
- 목표: 0.349 돌파

### 전략
- scale_pos_weight: 1, 5, 10, 20, 30 테스트
- 예측 평균이 0.0191에 가장 가까운 값 선택
- 확률 칼리브레이션 적용

---

## 실험 012: Deep CTR Model (진행 중)
**날짜**: 2025-09-16 22:10
**파일**: 012_deep_ctr_submission.csv (예정)

### 동기
- A100 GPU 활용한 딥러닝 모델
- Wide & Deep + Attention 구조
- 비선형 패턴 학습

### 모델 구조
- 범주형 임베딩 (6개 변수)
- Wide: 선형 변환
- Deep: 3층 신경망 [512, 256, 128]
- Attention 메커니즘

### 기대 효과
- 복잡한 상호작용 학습
- End-to-end 최적화
- 목표 AUC: 0.75+

---

## 핵심 인사이트

### 성공 요인
1. **예측 분산**: 표준편차 > 0.05 필요 (AP 향상)
2. **예측 평균**: 실제 클릭률(0.0191)에 근접 필요 (WLL 개선)
3. **모델 다양성**: XGBoost가 LightGBM보다 우수

### 실패 요인
1. **과도한 정규화**: Optuna의 min_gain_to_split 등
2. **클래스 불균형**: scale_pos_weight 조정 필요
3. **피처 엔지니어링**: Target Encoding 구현 주의

### 다음 시도
1. **앙상블**: XGBoost + Deep Learning
2. **피처 선택**: 상위 중요 피처만 사용
3. **후처리**: 확률 칼리브레이션 강화

---

## 제출 기록 요약

| 실험 | 파일명 | 리더보드 점수 | 주요 특징 |
|------|--------|--------------|-----------|
| 008 | submission.csv | 0.21436 | LightGBM, Optuna params, 낮은 분산 |
| 010 | 010_xgboost_submission.csv | 0.31631 | XGBoost, 높은 분산, 과도한 positive |
| 011 | 011_balanced_submission.csv | 진행 중 | CV로 균형 찾기 |
| 012 | 012_deep_ctr_submission.csv | 진행 중 | GPU 딥러닝 |

---

## 실험 023: Ultra Batch Deep Learning
**날짜**: 2025-09-17 10:00
**파일**: 023_ultra_batch_submission.csv

### 동기
- GPU 메모리 활용 극대화 (256K 배치 크기)
- Gradient Accumulation으로 1M 유효 배치

### 결과
- **리더보드 점수: 0.1574** ❌
- **예측 통계**:
  - 평균: 0.1446
  - 표준편차: 0.3054
- **문제점**: XGBoost보다 낮은 성능

---

## 앙상블 (025)
**날짜**: 2025-09-17 11:00

### 구성
- XGBoost (010): 0.31631
- Deep Learning (023): 0.1574
- 상관관계: 0.5442 (앙상블에 적합)

### 제출 결과
- **025_ensemble_conservative.csv**: Geometric mean - **점수: 0.2275** (2025-09-17 11:05:33)
- **025_ensemble_balanced.csv**: Weighted 60:40 - 대기 중
- **025_ensemble_harmonic.csv**: Harmonic mean - 대기 중
- **025_ensemble_rank.csv**: 순위 기반 - 대기 중

**목표**: 0.349 (현재 최고: 0.31631)